"""
Memory Tools: Retrieve and Save Long-term Memory
"""

import os
import logging
from typing import Optional, Type
from datetime import datetime

from langchain.tools import BaseTool
from langchain_openai import ChatOpenAI
from pydantic import BaseModel, Field
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct
from protonx import ProtonX

from opentelemetry import trace, metrics
from opentelemetry.trace import Status, StatusCode

from config import LONGTERM_COLLECTION_NAME

logger = logging.getLogger(__name__)


# ============================================================================
# Tool 1: Retrieve Long Term Memory
# ============================================================================

class RetrieveLongTermMemoryInput(BaseModel):
    """Input schema cho Retrieve Long Term Memory tool"""
    query: str = Field(
        description="CÃ¢u há»i hiá»‡n táº¡i Ä‘á»ƒ tÃ¬m kiáº¿m thÃ´ng tin liÃªn quan trong long-term memory"
    )


class RetrieveLongTermMemoryTool(BaseTool):
    """Tool Ä‘á»ƒ truy váº¥n thÃ´ng tin long-term tá»« file longterm.txt"""
    
    name: str = "retrieve_long_term_memory"
    description: str = """
    LUÃ”N LUÃ”N gá»i tool nÃ y Äáº¦U TIÃŠN trÆ°á»›c khi tráº£ lá»i báº¥t ká»³ cÃ¢u há»i nÃ o cá»§a ngÆ°á»i dÃ¹ng.
    Tool nÃ y truy váº¥n thÃ´ng tin cÃ¡ nhÃ¢n vÃ  dÃ i háº¡n Ä‘Ã£ lÆ°u vá» ngÆ°á»i dÃ¹ng Ä‘á»ƒ cÃ¡ nhÃ¢n hÃ³a cÃ¢u tráº£ lá»i.
    Giá»‘ng nhÆ° ChatGPT Personalization - luÃ´n kiá»ƒm tra xem cÃ³ thÃ´ng tin nÃ o há»¯u Ã­ch khÃ´ng.
    Input: CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng (khÃ´ng sá»­a Ä‘á»•i)
    Output: ThÃ´ng tin cÃ¡ nhÃ¢n liÃªn quan (náº¿u cÃ³)
    """
    args_schema: Type[BaseModel] = RetrieveLongTermMemoryInput
    longterm_temp_file: str = "longterm_temp.txt"
    longterm_file: str = "longterm.txt"
    llm: Optional[ChatOpenAI] = None
    openai_client: Optional[OpenAI] = None
    model_name: str = "gpt-4o-mini"
    
    def _run(self, query: str) -> str:
        """Thá»±c thi tool Ä‘á»ƒ láº¥y thÃ´ng tin tá»« long-term memory"""
        # Get tracer for this tool
        tracer = trace.get_tracer(__name__)
        meter = metrics.get_meter(__name__)
        
        # Create counter for tool invocations
        tool_counter = meter.create_counter(
            name="tool.invocations",
            description="Number of tool invocations",
            unit="1"
        )
        
        # Start span for this tool execution
        with tracer.start_as_current_span(
            "retrieve_long_term_memory",
            attributes={
                "tool.name": "retrieve_long_term_memory",
                "tool.input.query": query[:500] if query else "",  # Limit to 500 chars
                "tool.file": self.longterm_file
            }
        ) as span:
            try:
                logger.info(f"ðŸ” Retrieving long-term memory for query: {query[:50]}...")
                tool_counter.add(1, {"tool.name": "retrieve_long_term_memory", "status": "started"})
                if not os.path.exists(self.longterm_file):
                    output = "KhÃ´ng cÃ³ thÃ´ng tin long-term memory nÃ o Ä‘Æ°á»£c lÆ°u trá»¯."
                    span.set_attribute("memory.exists", False)
                    span.set_attribute("tool.output", output)
                    span.set_status(Status(StatusCode.OK))
                    logger.warning("âš ï¸ Long-term memory file not found")
                    tool_counter.add(1, {"tool.name": "retrieve_long_term_memory", "status": "no_file"})
                    return output
                
                span.set_attribute("memory.exists", True)
                
                with open(self.longterm_file, 'r', encoding='utf-8') as f:
                    longterm_content = f.read().strip()

                with open(self.longterm_temp_file, 'r', encoding='utf-8') as f:
                    longterm_temp_content = f.read().strip()

                longterm_content = longterm_content + "\n" + longterm_temp_content

                span.set_attribute("memory.content_length", len(longterm_content))
                
                if not longterm_content:
                    output = "Long-term memory trá»‘ng, chÆ°a cÃ³ thÃ´ng tin nÃ o Ä‘Æ°á»£c lÆ°u."
                    span.set_attribute("tool.output", output)
                    span.set_status(Status(StatusCode.OK))
                    logger.info("â„¹ï¸ Long-term memory is empty")
                    tool_counter.add(1, {"tool.name": "retrieve_long_term_memory", "status": "empty"})
                    return output
                
                if self.openai_client is None:
                    output = f"ThÃ´ng tin tá»« long-term memory:\n{longterm_content}"
                    span.set_attribute("llm.used", False)
                    span.set_attribute("tool.output", output[:500])  # Limit to 500 chars
                    span.set_status(Status(StatusCode.OK))
                    logger.info("âœ… Returning raw long-term memory (no LLM)")
                    tool_counter.add(1, {"tool.name": "retrieve_long_term_memory", "status": "success_no_llm"})
                    return output
            
                prompt_text = f"""
Báº¡n lÃ  má»™t trá»£ lÃ½ thÃ´ng minh. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  phÃ¢n tÃ­ch long-term memory vÃ  tÃ¬m cÃ¡c thÃ´ng tin liÃªn quan Ä‘áº¿n ngá»¯ cáº£nh hiá»‡n táº¡i.

NGá»® Cáº¢NH HIá»†N Táº I:
{query}

LONG-TERM MEMORY:
{longterm_content}

HÃ£y:
1. TÃ¬m cÃ¡c thÃ´ng tin trong long-term memory cÃ³ liÃªn quan Ä‘áº¿n ngá»¯ cáº£nh hiá»‡n táº¡i
2. TÃ³m táº¯t láº¡i thÃ nh má»™t Ä‘oáº¡n ngáº¯n gá»n, dá»… hiá»ƒu
3. Chá»‰ Ä‘Æ°a ra thÃ´ng tin thá»±c sá»± há»¯u Ã­ch vÃ  liÃªn quan

TÃ“M Táº®T:
"""
                
                span.set_attribute("llm.used", True)
                span.set_attribute("llm.model", self.model_name)
                span.set_attribute("llm.prompt_length", len(prompt_text))
                
                # Nested span for LLM call
                with tracer.start_as_current_span("openai.chat.completions") as llm_span:
                    try:
                        logger.info(f"ðŸ¤– Calling OpenAI API (model: {self.model_name})...")
                        
                        # Sá»­ dá»¥ng OpenAI client trá»±c tiáº¿p
                        response = self.openai_client.chat.completions.create(
                            model=self.model_name,
                            messages=[
                                {"role": "user", "content": prompt_text}
                            ],
                            temperature=0.7
                        )
                        
                        # Add LLM metrics to span
                        llm_span.set_attribute("llm.response.tokens.prompt", response.usage.prompt_tokens)
                        llm_span.set_attribute("llm.response.tokens.completion", response.usage.completion_tokens)
                        llm_span.set_attribute("llm.response.tokens.total", response.usage.total_tokens)
                        llm_span.set_attribute("llm.response.model", response.model)
                        llm_span.set_status(Status(StatusCode.OK))
                        
                        # TrÃ­ch xuáº¥t content tá»« response
                        content = response.choices[0].message.content
                        llm_span.set_attribute("llm.response.content_length", len(content))
                        
                        span.set_attribute("tool.output_length", len(content))
                        span.set_attribute("tool.output", content[:500])  # Limit to 500 chars
                        span.set_status(Status(StatusCode.OK))
                        logger.info(f"âœ… Long-term memory retrieved successfully (tokens: {response.usage.total_tokens})")
                        tool_counter.add(1, {"tool.name": "retrieve_long_term_memory", "status": "success_with_llm"})
                        
                        return content
                        
                    except Exception as llm_error:
                        llm_span.set_status(Status(StatusCode.ERROR, str(llm_error)))
                        llm_span.record_exception(llm_error)
                        logger.error(f"âŒ OpenAI API error: {str(llm_error)}")
                        tool_counter.add(1, {"tool.name": "retrieve_long_term_memory", "status": "llm_error"})
                        return f"ThÃ´ng tin tá»« long-term memory:\n{longterm_content}"
                
            except Exception as e:
                import traceback
                error_msg = f"Lá»—i khi truy váº¥n long-term memory: {str(e)}"
                span.set_status(Status(StatusCode.ERROR, str(e)))
                span.record_exception(e)
                logger.error(f"{error_msg}\n{traceback.format_exc()}")
                tool_counter.add(1, {"tool.name": "retrieve_long_term_memory", "status": "error"})
                return error_msg
    
    async def _arun(self, query: str) -> str:
        """Async version (khÃ´ng báº¯t buá»™c)"""
        return self._run(query)


# ============================================================================
# Tool 2: Save Memory
# ============================================================================

class SaveMemoryInput(BaseModel):
    """Input schema cho Save Memory tool"""
    information: str = Field(
        description="""
ThÃ´ng tin quan trá»ng cáº§n lÆ°u vÃ o long-term memory vá» ngÆ°á»i dÃ¹ng.
"""
    )


class SaveMemoryTool(BaseTool):
    """Tool Ä‘á»ƒ lÆ°u thÃ´ng tin quan trá»ng vÃ o long-term memory (file + Qdrant)"""
    
    name: str = "save_memory"
    description: str = """
    LÆ°u trá»¯ thÃ´ng tin dÃ i háº¡n vÃ  quan trá»ng vá» ngÆ°á»i dÃ¹ng vÃ o long-term memory.
    Chá»‰ gá»i tool nÃ y khi xuáº¥t hiá»‡n thÃ´ng tin tháº­t sá»± quan trá»ng, á»•n Ä‘á»‹nh theo thá»i gian vÃ  há»¯u Ã­ch cho cÃ¡ nhÃ¢n hÃ³a cÃ¡c tÆ°Æ¡ng tÃ¡c sau nÃ y.
    
    ThÃ´ng tin sáº½ Ä‘Æ°á»£c lÆ°u vÃ o:
    1. File longterm.txt (vá»›i timestamp)
    2. Qdrant database (Ä‘á»ƒ semantic search)
    
    Input: ThÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c trÃ­ch xuáº¥t vÃ  tÃ³m táº¯t ngáº¯n gá»n
    Output: XÃ¡c nháº­n Ä‘Ã£ lÆ°u
    
    VÃ­ dá»¥ sá»­ dá»¥ng:
    - User: "TÃ´i tÃªn lÃ  Minh, 45 tuá»•i" â†’ save_memory("NgÆ°á»i dÃ¹ng tÃªn lÃ  Minh, 45 tuá»•i")
    - User: "TÃ´i bá»‹ tiá»ƒu Ä‘Æ°á»ng type 2" â†’ save_memory("Bá»‹ bá»‡nh tiá»ƒu Ä‘Æ°á»ng type 2")
    - User: "Con tÃ´i 5 tuá»•i tÃªn Linh" â†’ save_memory("CÃ³ con gÃ¡i 5 tuá»•i tÃªn Linh")
    """
    args_schema: Type[BaseModel] = SaveMemoryInput
    longterm_file: str = "longterm_temp.txt"
    llm: Optional[ChatOpenAI] = None
    qdrant_client: Optional[QdrantClient] = None
    protonx_client: Optional[ProtonX] = None
    
    def _run(self, information: str) -> str:
        """LÆ°u thÃ´ng tin vÃ o long-term memory"""
        # Get tracer for this tool
        tracer = trace.get_tracer(__name__)
        meter = metrics.get_meter(__name__)
        
        # Create counter for tool invocations
        tool_counter = meter.create_counter(
            name="tool.invocations",
            description="Number of tool invocations",
            unit="1"
        )
        
        # Start span for this tool execution
        with tracer.start_as_current_span(
            "save_memory",
            attributes={
                "tool.name": "save_memory",
                "tool.input.information": information[:100] if information else "",  # Limit length
                "tool.file": self.longterm_file
            }
        ) as span:
            try:
                logger.info(f"ðŸ’¾ Saving to long-term memory: {information[:50]}...")
                tool_counter.add(1, {"tool.name": "save_memory", "status": "started"})
                
                if not information or not information.strip():
                    output = "KhÃ´ng cÃ³ thÃ´ng tin Ä‘á»ƒ lÆ°u."
                    span.set_attribute("save.success", False)
                    span.set_attribute("save.reason", "empty_information")
                    span.set_attribute("tool.output", output)
                    span.set_status(Status(StatusCode.OK))
                    logger.warning("âš ï¸ No information to save (empty)")
                    tool_counter.add(1, {"tool.name": "save_memory", "status": "empty"})
                    return output
                
                span.set_attribute("save.information_length", len(information))
                timestamp = self._get_timestamp()
                span.set_attribute("save.timestamp", timestamp)
                
                # Save to file
                with tracer.start_as_current_span("save_to_file") as file_span:
                    try:
                        with open(self.longterm_file, 'a', encoding='utf-8') as f:
                            f.write(f"[{timestamp}] {information.strip()}\n")
                        file_span.set_attribute("save.file.success", True)
                        file_span.set_status(Status(StatusCode.OK))
                        logger.info(f"âœ… Saved to file: {self.longterm_file}")
                    except Exception as e:
                        file_span.set_status(Status(StatusCode.ERROR, str(e)))
                        file_span.record_exception(e)
                        logger.error(f"âŒ Error saving to file: {str(e)}")
                        raise
                
                # Save to Qdrant database
                if self.qdrant_client and self.protonx_client:
                    with tracer.start_as_current_span("save_to_qdrant") as qdrant_span:
                        try:
                            logger.info("ðŸ”¤ Creating embedding for Qdrant...")
                            
                            # Create embedding
                            response = self.protonx_client.embeddings.create([information.strip()])
                            if isinstance(response, dict):
                                embedding = response["data"][0]["embedding"]
                            else:
                                embedding = response.data[0].embedding
                            
                            qdrant_span.set_attribute("embedding.dimension", len(embedding))
                            
                            # Get next ID (count existing points + 1)
                            collection_info = self.qdrant_client.get_collection(LONGTERM_COLLECTION_NAME)
                            next_id = collection_info.points_count + 1
                            
                            # Create point
                            point = PointStruct(
                                id=next_id,
                                vector={"default": embedding},
                                payload={
                                    "text": f"[{timestamp}] {information.strip()}",
                                    "text_without_timestamp": information.strip(),
                                    "timestamp": timestamp,
                                    "created_at": datetime.now().isoformat()
                                }
                            )
                            
                            # Upload to Qdrant
                            self.qdrant_client.upsert(
                                collection_name=LONGTERM_COLLECTION_NAME,
                                points=[point]
                            )
                            
                            qdrant_span.set_attribute("save.qdrant.success", True)
                            qdrant_span.set_attribute("save.qdrant.point_id", next_id)
                            qdrant_span.set_status(Status(StatusCode.OK))
                            logger.info(f"âœ… Saved to Qdrant database (ID: {next_id})")
                            
                        except Exception as e:
                            qdrant_span.set_status(Status(StatusCode.ERROR, str(e)))
                            qdrant_span.record_exception(e)
                            logger.warning(f"âš ï¸ Could not save to Qdrant: {str(e)}")
                            # Don't fail if Qdrant save fails - file save is primary
                else:
                    logger.warning("âš ï¸ Qdrant/ProtonX not available, skipping database save")
                
                output = f"ÄÃ£ lÆ°u: {information.strip()}"
                span.set_attribute("save.success", True)
                span.set_attribute("tool.output", output)
                span.set_status(Status(StatusCode.OK))
                logger.info(f"âœ… Successfully saved to long-term memory")
                tool_counter.add(1, {"tool.name": "save_memory", "status": "success"})
                
                return output
                
            except Exception as e:
                span.set_attribute("save.success", False)
                span.set_attribute("save.error", str(e))
                span.set_status(Status(StatusCode.ERROR, str(e)))
                span.record_exception(e)
                logger.error(f"âŒ Error saving to long-term memory: {str(e)}")
                tool_counter.add(1, {"tool.name": "save_memory", "status": "error"})
                return f"Lá»—i khi lÆ°u: {str(e)}"
    
    def _get_timestamp(self):
        """Láº¥y timestamp hiá»‡n táº¡i"""
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    async def _arun(self, information: str) -> str:
        """Async version"""
        return self._run(information)

